package com.yxt.bigdata.etl.connector.hive.writer

import com.typesafe.config.Config
import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
import org.apache.spark.sql.types.StructType
import com.yxt.bigdata.etl.connector.base.AdvancedConfig
import com.yxt.bigdata.etl.connector.base.component.ETLWriter


class HiveWriter(conf: Config) extends ETLWriter {
  override val tableName: String = AdvancedConfig.getString(conf, Key.TABLE_NAME)

  override var columns: Array[String] = {
    AdvancedConfig.getString(conf, Key.COLUMNS).split(",").map(_.trim.toLowerCase)
  }

  override val workersNum: Int = AdvancedConfig.getIntWithDefaultValue(conf, Key.WORKERS_NUM, Constant.WORKERS_NUM)

  override def saveTable(dataFrame: DataFrame): Unit = {
    val spark = dataFrame.sparkSession
    val originalTableName = tableName
    val tmpTableName = genTmpTableName(tableName)

    // 创建表
    createTableIfNotExists(spark, dataFrame.schema, originalTableName)

    // 存储格式：textfile、orc
    // textfile: dropDelims(dataFrame)
    // orc: 不需要指定考虑分隔符
    val dataFrameWithoutDelims = dataFrame
    dataFrameWithoutDelims.createOrReplaceTempView(tmpTableName)

    // 先保存到临时表
    // 防止出错重导、文件琐碎
//    saveTableAsTmp(dataFrame, tmpTableName)
    // 覆盖、合并操作
    spark.sql(s"insert overwrite table $originalTableName select * from $tmpTableName")
  }

  def saveTableAsTmp(dataFrame: DataFrame, tmpTableName: String): Unit = {
    val spark = dataFrame.sparkSession
    createTableIfNotExists(spark, dataFrame.schema, tmpTableName)

    // 存储格式：textfile、orc
    // textfile: dropDelims(dataFrame)
    // orc: 不需要指定考虑分隔符
    val dataFrameWithoutDelims = dataFrame
    dataFrameWithoutDelims.write.format("parquet").mode(SaveMode.Overwrite).saveAsTable(tmpTableName)
  }

  def genTmpTableName(originalTableName: String): String = {
    var tmpTableName: String = null
    val sepTableName = originalTableName.split("[.]")
    val len = sepTableName.length

    if (len == 1) tmpTableName = s"tmp_$originalTableName"
    else if (len == 2) {
      val Array(_, table) = sepTableName
      tmpTableName = s"tmp_$table"
    }
    else throw new Exception(s"表名配置信息有误，您的表名为：$originalTableName , 而目前支持的表名格式有两种：'database.table' 和 'table'，请检查您的配置并作出修改。")

    tmpTableName
  }

  def createTableIfNotExists(spark: SparkSession, schema: StructType, tableName: String): Unit = {
    val fields = schema.fields

    val sb = new StringBuilder()
    for (f <- fields) sb.append(s"${f.name} ${f.dataType.typeName},")
    val colString = sb.toString
    val createSql =
    //      s"""
    //         |CREATE TABLE IF NOT EXISTS
    //         |$tableName
    //         |(${colString.slice(0, colString.length - 1)})
    //         |ROW FORMAT DELIMITED
    //         |FIELDS TERMINATED BY '\001'
    //         |LINES TERMINATED BY '\n'
    //         |STORED AS TEXTFILE
    //       """.stripMargin
      s"""
         |CREATE TABLE IF NOT EXISTS
         |$tableName
         |(${colString.slice(0, colString.length - 1)})
         |STORED AS PARQUET
       """.stripMargin
    spark.sql(createSql)
  }

  def dropDelims(dataFrame: DataFrame): DataFrame = {
    val schema = dataFrame.schema
    val fields = schema.fields
    val exprs = new Array[String](fields.length)
    for (i <- fields.indices) {
      val field = fields(i)
      if ("string".equals(field.dataType.typeName)) {
        exprs(i) = s"regexp_replace(${field.name}, '\\r|\\n|\\001', '') AS ${field.name}"
      } else exprs(i) = s"${field.name}"
    }
    dataFrame.selectExpr(exprs: _*)
  }
}
