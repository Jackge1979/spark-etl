package com.yxt.bigdata.etl

import org.apache.spark.sql.SparkSession


object MainHandler {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Spark-ETL")
      .master("local[4]")
      .enableHiveSupport()
      .getOrCreate()

    val job = new JobParser("mysqlToMysql.json")
    val writer = job.getWriter
    for (reader <- job.getReaders) {
      // 数据库查询
      var df = reader.getDataFrame(spark)

      val readerColumns = reader.columns
      val writerColumns = writer.columns
      if (readerColumns.length != writerColumns.length) {
        throw new Exception("列配置信息有误，因为您配置的任务中，源头读取字段数：%d 与 目标表要写入的字段数：%d 不相等，请检查您的配置并作出修改。")
      }

      val columnsWithAlias = new Array[String](readerColumns.length)
      for (i <- readerColumns.indices) {
        val rc = readerColumns(i)
        val wc = writerColumns(i)
        if (rc.equals(wc)) columnsWithAlias(i) = s"$rc"
        else columnsWithAlias(i) = s"$rc AS $wc"
      }
      df = df.selectExpr(columnsWithAlias: _*)

      writer.saveTable(df)

      //          df.write
      //            .mode(SaveMode.Overwrite)
      //            .option("header", "true")
      //            .csv("/tmp/org")

    }


  }
}