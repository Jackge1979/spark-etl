package com.yxt.bigdata.etl.connector.rdbms.reader

import scala.util.Random

import com.typesafe.config.Config
import org.apache.spark.sql.{Column, DataFrame, SparkSession}

import com.yxt.bigdata.etl.connector.base.component.ETLReader
import com.yxt.bigdata.etl.connector.base.db.DBUtil
import com.yxt.bigdata.etl.connector.base.AdvancedConfig


class RdbReader(conf: Config) extends DBUtil(conf) with ETLReader {
  override val tableName: String = AdvancedConfig.getString(conf, Key.TABLE_NAME)

  override var columns: Option[Array[String]] = {
    val cols = AdvancedConfig.getString(conf, Key.COLUMNS, isNecessary = false)
    cols.map(_.split(",").map(_.trim.toLowerCase))
  }

  override var where: Option[String] = AdvancedConfig.getString(conf, Key.WHERE, isNecessary = false)

  override val querySql: Option[String] = AdvancedConfig.getString(conf, Key.QUERY_SQL, isNecessary = false)

  val predicatesConf: Option[Config] = AdvancedConfig.getConfig(conf, Key.PREDICATES, isNecessary = false)

  override def getDataFrame(spark: SparkSession): DataFrame = {
    var df: DataFrame = null

    predicatesConf.foreach(predConf => {
      val typeName = AdvancedConfig.getString(predConf, Key.PREDICATES_TYPE)
      typeName match {
        case "long" =>
          /*
          多并发
          按数字字段切分
           */
          val (columnName, lowerBound, upperBound, numPartitions) = PredicatesParser.parseLongFieldPredicates(predConf)
          df = spark.read.jdbc(
            jdbcUrl,
            tableName,
            columnName,
            lowerBound,
            upperBound,
            numPartitions,
            jdbcProperties
          )
        case "custom" =>
          /*
          多并发
          自定义切分规则
           */
          val predicates = PredicatesParser.parseCustomPredicates(predConf)
          df = spark.read.jdbc(
            jdbcUrl,
            tableName,
            predicates,
            jdbcProperties
          )
        case _ =>
          /*
          单并发
           */
          df = spark.read.jdbc(
            jdbcUrl,
            tableName,
            jdbcProperties
          )
      }
    })

    // 列名最小化
    val loweredColumns = df.columns.map(col => col.toLowerCase)
    df.toDF(loweredColumns: _*)
    // 输出schema
    df.printSchema()

    // query sql
    querySql.foreach(query => {
      val tmpTableName = Random.nextString(10)
      df.createOrReplaceTempView(tmpTableName)
      df = spark.sql(query)

      columns = Option(null)
      where = Option(null)
    })
    // columns
    columns.foreach(cols => df = df.selectExpr(cols: _*))
    // where
    where.foreach(condition => df = df.filter(condition))

    df
  }
}
