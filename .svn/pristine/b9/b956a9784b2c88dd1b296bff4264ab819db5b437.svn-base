package com.yxt.bigdata.etl.connector.rdbms.reader

import com.typesafe.config.Config
import org.apache.spark.sql.{DataFrame, SparkSession}

import com.yxt.bigdata.etl.connector.base.ETLReader
import com.yxt.bigdata.etl.connector.base.db.DBUtil


class RdbReader(conf: Config) extends DBUtil(conf) with ETLReader {
  val tableName: String = conf.getString(Key.TABLE_NAME)

  val columns: Array[String] = {
    val cols = conf.getString(Key.COLUMNS).toLowerCase
    if (!"*".equals(cols)) cols.split(",")
    else getAllColumns(tableName)
  }

  def getPredicates: Array[String] = {
    //    val stringPredicates = conf.getString(Key.PREDICATES)
    //    stringPredicates.split(",")

    // 分区，orgid首字母
    val predicates = for {
      i <- (48 to 57) ++ (97 to 122)
      prefix = i.toChar.toString
    } yield s"substr(pid, 1, 1)='$prefix'"
    predicates.toArray
  }

  override def getDataFrame(spark: SparkSession, tableName: String): DataFrame = {
    val df = spark.read.jdbc(
      jdbcUrl,
      tableName,
      getPredicates,
      jdbcProperties
    )

    val loweredColumns = df.columns.map(col => col.toLowerCase)
    df.toDF(loweredColumns: _*)
  }
}
