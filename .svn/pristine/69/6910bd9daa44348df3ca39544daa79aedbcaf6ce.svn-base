package com.yxt.bigdata.etl.connector.hive.writer

import java.sql.DriverManager

import com.typesafe.config.Config
import org.apache.spark.sql.{DataFrame, Row, SparkSession, SaveMode}

import com.yxt.bigdata.etl.connector.base.AdvancedConfig
import com.yxt.bigdata.etl.connector.base.component.ETLWriter


class HiveWriter(conf: Config) extends ETLWriter {
  override val tableName: String = AdvancedConfig.getString(conf, Key.TABLE_NAME)

  override var columns: Array[String] = {
    AdvancedConfig.getString(conf, Key.COLUMNS).split(",").map(_.trim.toLowerCase)
  }

  override val workersNum: Int = AdvancedConfig.getIntWithDefaultValue(conf, Key.WORKERS_NUM, Constant.WORKERS_NUM)

  override def saveTable(dataFrame: DataFrame): Unit = {
    val spark = dataFrame.sparkSession

    createTable(dataFrame)
    // 存储格式：textfile、orc
    // 如果是textfile，需要dropDelims(dataFrame)；如果指定orc格式，则不需要指定考虑分隔符
    val dataFrameWithoutDelims = dataFrame

    val tmpTableName = genTmpTableName(tableName)
    dataFrameWithoutDelims.createOrReplaceTempView(tmpTableName)
    spark.sql(s"insert overwrite table $tableName select * from $tmpTableName")
    //    dataFrameWithoutDelims.write.format("orc").mode(SaveMode.Overwrite).saveAsTable(tableName)
  }

  def genTmpTableName(originalTableName: String): String = {
    var tmpTableName: String = null
    val sepTableName = originalTableName.split("[.]")
    val len = sepTableName.length

    if (len == 1) tmpTableName = s"tmp_$originalTableName"
    else if (len == 2) {
      val Array(_, table) = sepTableName
      tmpTableName = s"tmp_$table"
    }
    else throw new Exception(s"表名配置信息有误，您的表名为：$originalTableName , 而目前支持的表名格式有两种：'database.table' 和 'table'，请检查您的配置并作出修改。")

    tmpTableName
  }

  def createTable(dataFrame: DataFrame): Unit = {
    val schema = dataFrame.schema
    val fields = schema.fields

    val sb = new StringBuilder()
    for (f <- fields) sb.append(s"${f.name} ${f.dataType.typeName},")
    val colString = sb.toString
    val createSql =
    //      s"""
    //         |CREATE TABLE IF NOT EXISTS
    //         |$tableName
    //         |(${colString.slice(0, colString.length - 1)})
    //         |ROW FORMAT DELIMITED
    //         |FIELDS TERMINATED BY '\001'
    //         |LINES TERMINATED BY '\n'
    //         |STORED AS TEXTFILE
    //       """.stripMargin
      s"""
         |CREATE TABLE IF NOT EXISTS
         |$tableName
         |(${colString.slice(0, colString.length - 1)})
         |STORED AS ORC
       """.stripMargin
    dataFrame.sparkSession.sql(createSql)
  }

  def dropDelims(dataFrame: DataFrame): DataFrame = {
    val schema = dataFrame.schema
    val fields = schema.fields
    val exprs = new Array[String](fields.length)
    for (i <- fields.indices) {
      val field = fields(i)
      if ("string".equals(field.dataType.typeName)) {
        exprs(i) = s"regexp_replace(${field.name}, '\\r|\\n|\\001', '') AS ${field.name}"
      } else exprs(i) = s"${field.name}"
    }
    dataFrame.selectExpr(exprs: _*)
  }
}
