package com.yxt.bigdata.etl.connector.hive.writer

import com.typesafe.config.Config
import org.apache.spark.sql.{DataFrame, SaveMode}
import com.yxt.bigdata.etl.connector.base.ETLWriter
import com.yxt.bigdata.etl.connector.base.db.DBUtil

class HiveWriter(conf: Config) extends DBUtil(conf) with ETLWriter {
  override val tableName: String = conf.getString(Key.TABLE_NAME)

  override val columns: Array[String] = {
    val cols = conf.getString(Key.COLUMNS).toLowerCase
    cols.split(",")
  }

  override def saveTable(dataFrame: DataFrame): Unit = {
    dataFrame.write
      .mode("append")
      .format("hive")
      .saveAsTable(tableName)
  }
}
