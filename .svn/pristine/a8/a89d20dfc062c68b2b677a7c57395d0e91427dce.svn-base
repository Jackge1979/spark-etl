package com.yxt.bigdata.etl.connector.hive.reader

import com.typesafe.config.Config
import com.yxt.bigdata.etl.connector.base.AdvancedConfig
import com.yxt.bigdata.etl.connector.base.component.ETLReader
import org.apache.spark.sql.{DataFrame, SparkSession}


class HiveReader(conf: Config) extends ETLReader {
  override val tableName: String = AdvancedConfig.getString(conf, Key.TABLE_NAME)

  override var columns: Option[Array[String]] = {
    val cols = AdvancedConfig.getString(conf, Key.COLUMNS, isNecessary = false)
    cols.map(_.split(",").map(_.trim.toLowerCase))
  }

  override var where: Option[String] = AdvancedConfig.getString(conf, Key.WHERE, isNecessary = false)

  override val querySql: Option[String] = AdvancedConfig.getString(conf, Key.QUERY_SQL, isNecessary = false)

  override def getDataFrame(spark: SparkSession): DataFrame = {
    var df: DataFrame = null

    // query sql
    querySql.foreach(query => {
      df = spark.sql(query)

      columns = Option(null)
      where = Option(null)
    })
    // columns
    columns.foreach(cols => df = spark.sql(s"select ${cols.mkString(",")} from $tableName"))
    // where
    where.foreach(condition => df = df.filter(condition))

    df
  }
}
